import os
from langchain_community.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
import subprocess

# Chargement des vecteurs
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
db = Chroma(persist_directory="./chroma_index", embedding_function=embedding_model)

# Historique de la session (m√©moire temporaire)
historique = []

print("üß† Euno√Øa est pr√™te. Pose-lui tes questions (ou tape 'exit' pour quitter).\n")

while True:
    question = input("üí¨ Toi : ")

    if question.strip().lower() in ["exit", "quit"]:
        print("üëã √Ä bient√¥t !")
        break

    # Recherche vectorielle
    docs = db.similarity_search(question, k=3)
    contexte = "\n\n".join([doc.page_content for doc in docs])

    # Historique format√©
    historique_formate = "\n".join(historique[-3:])  # limite √† 3 derniers tours

    # Construction du prompt
    prompt = f"""
Tu es Euno√Øa, une intelligence artificielle douce et comp√©tente sp√©cialis√©e dans Linux.

Voici le contexte extrait de ta base de connaissances :
{contexte}

---

Voici les derniers √©changes :
{historique_formate}

---

R√©ponds maintenant √† la question suivante :
{question}
"""

    # Appel √† Gemma 3 via Ollama
    print("ü§ñ Euno√Øa : ", end="", flush=True)
    process = subprocess.Popen(["ollama", "run", "gemma3"], stdin=subprocess.PIPE, text=True)
    process.communicate(prompt)

    # Ajout √† l'historique
    historique.append(f"Toi : {question}")
    historique.append(f"Euno√Øa : [r√©ponse g√©n√©r√©e]")

